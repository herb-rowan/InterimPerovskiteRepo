{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training RF and Gradient Boosting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"perovskite_database_query.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = [\n",
    "    'Cell_stack_sequence', 'Cell_architecture',\n",
    "    'Substrate_stack_sequence', 'Substrate_thickness',\n",
    "    'ETL_stack_sequence', 'ETL_thickness', 'ETL_additives_compounds', 'ETL_additives_concentrations',\n",
    "    'Perovskite_composition_a_ions', 'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions', 'Perovskite_composition_b_ions_coefficients',\n",
    "    'Perovskite_composition_c_ions', 'Perovskite_composition_c_ions_coefficients', \n",
    "    'Perovskite_additives_compounds', 'Perovskite_additives_concentrations', 'Perovskite_thickness',\n",
    "    'HTL_stack_sequence', 'HTL_thickness_list', 'HTL_additives_compounds', 'HTL_additives_concentrations',\n",
    "    'Backcontact_stack_sequence', 'Backcontact_thickness', \n",
    "    'Backcontact_additives_compounds', 'Backcontact_additives_concentrations',\n",
    "    'Add_lay_front', 'Add_lay_front_function', 'Add_lay_front_stack_sequence', 'Add_lay_front_thickness_list', \n",
    "    'Add_lay_front_additives_compounds', 'Add_lay_front_additives_concentrations',\n",
    "    'Add_lay_back', 'Add_lay_back_function', 'Add_lay_back_stack_sequence', 'Add_lay_back_thickness_list', \n",
    "    'Add_lay_back_additives_compounds', 'Add_lay_back_additives_concentrations',\n",
    "    'Encapsulation', 'Encapsulation_stack_sequence',\n",
    "    'JV_default_PCE', 'JV_default_Voc', 'JV_default_Jsc', 'JV_default_FF'\n",
    "  # Added the target column\n",
    "]\n",
    "\n",
    "# Filter columns to keep only those that exist in the dataset\n",
    "existing_columns = [col for col in columns_to_keep if col in data.columns]\n",
    "data = data[existing_columns]\n",
    "\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Function to clean molecule names\n",
    "def clean_molecule_name(name):\n",
    "    name = re.sub(r'[^a-zA-Z0-9\\s\\-()]+', ' ', name.strip())\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    elements = [element for element in name.split() if element and not element.replace('.', '', 1).isdigit()]\n",
    "    return elements\n",
    "\n",
    "# Function to clean and convert coefficients to floats\n",
    "def clean_and_convert_coefficient(coefficient):\n",
    "    try:\n",
    "        cleaned_coefficient = re.sub(r'[^0-9.eE-]', '', coefficient.replace(',', '').strip())\n",
    "        return float(cleaned_coefficient) if cleaned_coefficient else 0.0\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "# Function to normalize coefficients\n",
    "def normalize_coefficients(cell):\n",
    "    if pd.notna(cell):\n",
    "        try:\n",
    "            coefficients = [float(x.strip()) for x in re.split(r'[;|]', cell) if x.strip()]\n",
    "            total_sum = sum(coefficients)\n",
    "            return ';'.join(f\"{val / total_sum:.3f}\" for val in coefficients) if total_sum > 0 else cell\n",
    "        except ValueError:\n",
    "            return cell\n",
    "    return cell\n",
    "\n",
    "# Normalize coefficients in each column\n",
    "coefficient_columns = [\n",
    "    'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions_coefficients', \n",
    "    'Perovskite_composition_c_ions_coefficients'\n",
    "]\n",
    "\n",
    "for col in coefficient_columns:\n",
    "    data[col] = data[col].apply(normalize_coefficients)\n",
    "\n",
    "# Create a set of unique molecules and add new columns\n",
    "unique_molecules = set()\n",
    "for index, row in data.iterrows():\n",
    "    for column_group in ['a', 'b', 'c']:\n",
    "        ions_column = f'Perovskite_composition_{column_group}_ions'\n",
    "        coefficients_column = f'Perovskite_composition_{column_group}_ions_coefficients'\n",
    "        ions = clean_molecule_name(str(row.get(ions_column, \"\")))\n",
    "        coefficients = [clean_and_convert_coefficient(c) for c in str(row.get(coefficients_column, \"\")).split(';')]\n",
    "        unique_molecules.update(ions)\n",
    "\n",
    "# Create columns for each unique molecule and initialize to zero\n",
    "for molecule in unique_molecules:\n",
    "    data[molecule] = 0.0\n",
    "\n",
    "# Populate the molecule columns with coefficients\n",
    "for index, row in data.iterrows():\n",
    "    for column_group in ['a', 'b', 'c']:\n",
    "        ions_column = f'Perovskite_composition_{column_group}_ions'\n",
    "        coefficients_column = f'Perovskite_composition_{column_group}_ions_coefficients'\n",
    "        ions = clean_molecule_name(str(row.get(ions_column, \"\")))\n",
    "        coefficients = [clean_and_convert_coefficient(c) for c in str(row.get(coefficients_column, \"\")).split(';')]\n",
    "        total_coeff = sum(coefficients) if sum(coefficients) != 0 else 1\n",
    "        for ion, coeff in zip(ions, coefficients):\n",
    "            data.at[index, ion] += coeff / total_coeff\n",
    "\n",
    "# Create a new column 'Layer Type' to indicate if the row is multilayered or single-layered\n",
    "ion_columns = [\n",
    "    'Perovskite_composition_a_ions', 'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions', 'Perovskite_composition_b_ions_coefficients',\n",
    "    'Perovskite_composition_c_ions', 'Perovskite_composition_c_ions_coefficients'\n",
    "]\n",
    "\n",
    "data['Layer Type'] = data.apply(\n",
    "    lambda row: 'Multi-layered Perovskite' if any('|' in str(row[col]) for col in ion_columns) else 'Single-layered Perovskite',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Add combined ions and coefficients columns\n",
    "data['combined_ions'] = data.apply(\n",
    "    lambda row: f\"{row.get('Perovskite_composition_a_ions', '')},{row.get('Perovskite_composition_b_ions', '')},{row.get('Perovskite_composition_c_ions', '')}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "data['combined_coefficients'] = data.apply(\n",
    "    lambda row: f\"{row.get('Perovskite_composition_a_ions_coefficients', '')},{row.get('Perovskite_composition_b_ions_coefficients', '')},{row.get('Perovskite_composition_c_ions_coefficients', '')}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert combined columns to lists and clean coefficients\n",
    "import re\n",
    "\n",
    "def convert_to_list(entry):\n",
    "    if isinstance(entry, str):\n",
    "        entry = re.sub(r'[;|]', ',', entry)\n",
    "        return [item.strip() for item in entry.split(',') if item.strip()]\n",
    "    elif isinstance(entry, list):\n",
    "        return entry\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def safe_convert_to_float(entry):\n",
    "    try:\n",
    "        return float(entry)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def generate_combined_sites(row):\n",
    "    ions = row['combined_ions']\n",
    "    coefficients = row['combined_coefficients']\n",
    "    sites = []\n",
    "    site_labels = ['a', 'b', 'c']\n",
    "    for site, ions_col, coeff_col in zip(site_labels, \n",
    "                                         ['Perovskite_composition_a_ions', 'Perovskite_composition_b_ions', 'Perovskite_composition_c_ions'], \n",
    "                                         ['Perovskite_composition_a_ions_coefficients', 'Perovskite_composition_b_ions_coefficients', 'Perovskite_composition_c_ions_coefficients']):\n",
    "        num_ions = len(clean_molecule_name(str(row.get(ions_col, \"\"))))\n",
    "        num_coefficients = len(str(row.get(coeff_col, \"\")).split(';'))\n",
    "        sites.extend([site] * max(num_ions, num_coefficients))\n",
    "    return sites\n",
    "\n",
    "data['combined_ions'] = data['combined_ions'].apply(convert_to_list)\n",
    "data['combined_coefficients'] = data['combined_coefficients'].apply(\n",
    "    lambda x: [safe_convert_to_float(item) for item in convert_to_list(x)]\n",
    ")\n",
    "data['combined_sites'] = data.apply(generate_combined_sites, axis=1)\n",
    "\n",
    "def clean_coefficients(coefficients):\n",
    "    cleaned = []\n",
    "    for c in coefficients:\n",
    "        if isinstance(c, float):\n",
    "            cleaned.append(c)\n",
    "        elif isinstance(c, str) and c.replace('.', '', 1).isdigit():\n",
    "            cleaned.append(float(c))\n",
    "        else:\n",
    "            cleaned.append(0.0)\n",
    "    return cleaned\n",
    "\n",
    "def normalize_coefficients_within_cell(row):\n",
    "    ions = row['combined_ions']\n",
    "    coefficients = row['combined_coefficients']\n",
    "    sites = row['combined_sites']\n",
    "    site_a_coeffs = []\n",
    "    site_b_coeffs = []\n",
    "    site_c_coeffs = []\n",
    "    for coeff, site in zip(coefficients, sites):\n",
    "        try:\n",
    "            coeff = float(coeff)\n",
    "        except ValueError:\n",
    "            coeff = 0.0\n",
    "        if site == 'a':\n",
    "            site_a_coeffs.append(coeff)\n",
    "        elif site == 'b':\n",
    "            site_b_coeffs.append(coeff)\n",
    "        elif site == 'c':\n",
    "            site_c_coeffs.append(coeff)\n",
    "    def normalize(site_coeffs):\n",
    "        total = sum(site_coeffs)\n",
    "        return [coeff / total if total > 0 else 0.0 for coeff in site_coeffs]\n",
    "    site_a_coeffs = normalize(site_a_coeffs)\n",
    "    site_b_coeffs = normalize(site_b_coeffs)\n",
    "    site_c_coeffs = normalize(site_c_coeffs)\n",
    "    normalized_coeffs = site_a_coeffs + site_b_coeffs + site_c_coeffs\n",
    "    return normalized_coeffs\n",
    "\n",
    "data['combined_coefficients'] = data['combined_coefficients'].apply(clean_coefficients)\n",
    "data['combined_coefficients'] = data.apply(normalize_coefficients_within_cell, axis=1)\n",
    "\n",
    "# Drop the original ion columns\n",
    "data = data.drop(columns=ion_columns, errors='ignore')\n",
    "\n",
    "# Save the modified DataFrame\n",
    "output_file_path = 'data_with_layer_type_and_combined.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "print(\"CSV file with layer type information modified and saved as:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for vecotorised, with molecules and JV_default_Voc', 'JV_default_Jsc','JV_default_FF'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "def load_and_filter_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified CSV file and filters rows where 'Cell_architecture' is 'nip'.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data[data['Cell_architecture'].str.strip().str.lower() == 'nip']\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "# 2. Define Layer Columns\n",
    "def define_layer_columns():\n",
    "    \"\"\"\n",
    "    Defines the mapping between stack sequence columns and their corresponding layer names.\n",
    "    \"\"\"\n",
    "    layer_columns = {\n",
    "        'Cell_stack_sequence': 'Cell',\n",
    "        'Substrate_stack_sequence': 'Substrate',\n",
    "        'ETL_stack_sequence': 'ETL',\n",
    "        'HTL_stack_sequence': 'HTL',\n",
    "        'Backcontact_stack_sequence': 'Backcontact',\n",
    "        'Add_lay_back_stack_sequence': 'Add_Lay_Back',\n",
    "        'Encapsulation_stack_sequence': 'Encapsulation'\n",
    "    }\n",
    "    return layer_columns\n",
    "\n",
    "# 3. Parse Sequences from Multiple Columns\n",
    "def parse_sequences_from_columns(dataframe, layer_columns):\n",
    "    \"\"\"\n",
    "    Parses material sequences from multiple layer-specific columns and maps materials to their layers.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    material_layer_map = {}\n",
    "    layer_names = list(layer_columns.values())\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sequence = []\n",
    "        for col, layer_name in layer_columns.items():\n",
    "            seq_str = row.get(col, \"\")\n",
    "            if pd.isna(seq_str) or not seq_str.strip():\n",
    "                continue\n",
    "            sub_layers = seq_str.split(' | ')\n",
    "            for sub_layer in sub_layers:\n",
    "                materials = [material.strip() for material in sub_layer.split('; ') if material.strip()]\n",
    "                sequence.extend(materials)\n",
    "                for material in materials:\n",
    "                    if material not in material_layer_map:\n",
    "                        material_layer_map[material] = {}\n",
    "                    if layer_name not in material_layer_map[material]:\n",
    "                        material_layer_map[material][layer_name] = 0\n",
    "                    material_layer_map[material][layer_name] += 1\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences, material_layer_map, layer_names\n",
    "\n",
    "# 4. Train Word2Vec Model\n",
    "def train_word2vec(sequences, vector_size=50, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the provided material sequences.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=sequences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 5. Aggregate Embeddings for Each Sample\n",
    "def aggregate_embeddings(sequences, model, vector_size=50):\n",
    "    \"\"\"\n",
    "    Aggregates material embeddings for each sample by averaging.\n",
    "    \"\"\"\n",
    "    aggregated_features = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) == 0:\n",
    "            aggregated_features.append(np.zeros(vector_size))\n",
    "            continue\n",
    "        vectors = [model.wv[material] for material in seq if material in model.wv]\n",
    "        if vectors:\n",
    "            aggregated = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            aggregated = np.zeros(vector_size)\n",
    "        aggregated_features.append(aggregated)\n",
    "    return np.array(aggregated_features)\n",
    "\n",
    "# 6. Prepare Features and Targets\n",
    "def prepare_features_targets(aggregated_features, dataframe, target_column='JV_default_PCE'):\n",
    "    \"\"\"\n",
    "    Prepares the feature matrix and target vector for model training.\n",
    "    Includes embeddings and specified molecule columns.\n",
    "    \"\"\"\n",
    "    # Molecule columns as a list\n",
    "    molecule_columns = [\n",
    "        '(DAP)', '(PEI)', '(ThFA)', 'Sn', 'S', 'Tb', 'Sm', 'TN', '(PPA)', '(PDMA)', '(FEA)', \n",
    "        '(PyrEA)', 'OA', '(PBA)', '(PTA)', '(CPEA)', '(TEA)', '(mF1PEA)', 'FA', '(BI)', 'IM', \n",
    "        '(oF1PEA)', '(PA)', '(iPA)', 'Mg', 'Y', 'PR', '(PF6)', '(ODA)', 'F', 'BU', '(Ada)', \n",
    "        'Ca', 'NEA', '(SCN)', '(N-EtPy)', 'HA', '(MIC1)', 'Br', '(AVA)', '((CH3)3S)', '(BIM)', \n",
    "        'Mn', 'MA', '(4AMP)', '(A43)', '(CH3)3S', '(PPEA)', '(F5PEA)', '(C4H9N2H6)', '(5-AVAI)', \n",
    "        'Sr', '(DMA)', 'CA', 'Al', '(NH4)', '(4AMPY)', 'PN', 'Sb', '(PDA)', '(ALA)', 'Nb', 'Te', \n",
    "        'TA', '(MTEA)', '(Cl-PEA)', '(iso-BA)', '(DPA)', '(BYA)', 'DA', 'Bi', '(HTAB)', 'AN', \n",
    "        'NMABr', '(CHMA)', '(F3EA)', 'In', '(6-ACA)', 'GU', '(ImEA)', '(HEA)', 'IA', 'Aa', \n",
    "        '(APMim)', '(C8H17NH3)', '(Br-PEA)', 'PMA', '(MIC2)', '(PGA)', 'I', '(5-AVA)', '(PEA)', \n",
    "        'K', '(BEA)', '(PMA)', 'Eu', 'Cl', '(3AMP)', '(F-PEA)', '(C6H4NH2)', '(CH3ND3)', '(4FPEA)', \n",
    "        '(DAT)', '(Anyl)', '(TBA)', '(4ApyH)', 'Ba', '(pF1PEA)', '(TMA)', 'Rb', '(3AMPY)', '(IEA)', \n",
    "        '(nan)', '(NMA)', 'Ni', '(pFPEA)', '(BE)', '(EU-pyP)', '(PyEA)', '(BzDA)', 'Co', '(Ace)', \n",
    "        'Hg', 'Pb', '(EDA)', '(oFPEA)', 'Bn', '(f-PEA)', '(C4H9NH3)', '(CIEA)', '(mFPEA)', 'BA', \n",
    "        'DI', '(HdA)', '(PDA)', '(GABA)', 'Cu', 'PA', '(DMA)', 'Na', '(EPA)', '(OdA)', '(THM)', \n",
    "        'Ge', '(HDA)', '(BF4)', '(FPEA)', '(MIC3)', 'GA', '(ThMA)', 'Cs', '(BZA)', 'Au', '(H-PEA)', \n",
    "        'Ag', '(SCN)', '(TFEA)', 'EA', 'FPEAI', 'Fe', '(n-C3H7NH3)', '(BdA)', '(EDA)', 'BDA', 'Cr', \n",
    "        'Pt', 'Ti', '(C6H13NH3)', '(HAD)', 'Li', '(BDA)', 'O', 'La', 'Zn', 'JV_default_Voc', 'JV_default_Jsc','JV_default_FF'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist in the dataframe\n",
    "    existing_molecule_columns = [col for col in molecule_columns if col in dataframe.columns]\n",
    "    missing_columns = [col for col in molecule_columns if col not in dataframe.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"The following specified molecule columns are not in the dataframe and will be skipped: {missing_columns}\")\n",
    "    \n",
    "    # Extract the specified molecule columns from the dataframe\n",
    "    molecule_features = dataframe[existing_molecule_columns]\n",
    "    \n",
    "    # Handle missing values in molecule features\n",
    "    molecule_features = molecule_features.fillna(0.0)\n",
    "    \n",
    "    # Combine embeddings and molecule features\n",
    "    combined_features = np.hstack([aggregated_features, molecule_features.values])\n",
    "    \n",
    "    # Handle missing values in the combined features\n",
    "    combined_features = np.nan_to_num(combined_features, nan=0.0)\n",
    "    \n",
    "    # Combine features and target into a DataFrame for easier handling\n",
    "    feature_df = pd.DataFrame(combined_features)\n",
    "    target_series = dataframe[target_column]\n",
    "    \n",
    "    # Concatenate features and target\n",
    "    combined_df = pd.concat([feature_df, target_series], axis=1)\n",
    "    \n",
    "    # Drop rows where target is NaN\n",
    "    initial_count = combined_df.shape[0]\n",
    "    combined_df = combined_df.dropna(subset=[target_column])\n",
    "    final_count = combined_df.shape[0]\n",
    "    dropped = initial_count - final_count\n",
    "    if dropped > 0:\n",
    "        print(f\"Dropped {dropped} samples due to NaN in target '{target_column}'.\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = combined_df.drop(columns=[target_column]).values\n",
    "    y = combined_df[target_column].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 7. Train and Evaluate Models\n",
    "def train_evaluate_models_with_grid_search(X, y, cv=3):\n",
    "    \"\"\"\n",
    "    Trains and evaluates models using GridSearchCV for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 150],\n",
    "                'max_depth': [None, 10],\n",
    "                'min_samples_split': [2, 4],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 150],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 4],\n",
    "                'min_samples_split': [2, 4]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    results = []\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=config['model'],\n",
    "            param_grid=config['params'],\n",
    "            cv=cv,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        best_params = grid_search.best_params_\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "        y_pred = best_estimator.predict(X)\n",
    "        \n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Parameters': best_params,\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'R2': r2\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name} Best Params: {best_params}\")\n",
    "        print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 8. Save Results\n",
    "def save_results_to_csv(results, filename='model_results.csv'):\n",
    "    \"\"\"\n",
    "    Saves the model training results to a CSV file.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to save.\")\n",
    "        return\n",
    "    \n",
    "    keys = results[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(results)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# 9. Main Execution Function\n",
    "def main():\n",
    "    # File path to the CSV dataset\n",
    "    file_path = 'data_with_layer_type_and_combined.csv'\n",
    "    \n",
    "    # Load and filter data\n",
    "    data = load_and_filter_data(file_path)\n",
    "    print(f\"Loaded data with {data.shape[0]} samples.\")\n",
    "    \n",
    "    # Define layer columns\n",
    "    layer_columns = define_layer_columns()\n",
    "    \n",
    "    # Parse sequences\n",
    "    tokenized_sequences, material_layer_map, layer_names = parse_sequences_from_columns(data, layer_columns)\n",
    "    print(\"Parsed sequences from columns.\")\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = train_word2vec(tokenized_sequences)\n",
    "    print(\"Trained Word2Vec model.\")\n",
    "    \n",
    "    # Aggregate embeddings\n",
    "    aggregated_features = aggregate_embeddings(tokenized_sequences, model)\n",
    "    print(\"Aggregated embeddings for each sample.\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X, y = prepare_features_targets(aggregated_features, data)\n",
    "    print(f\"Prepared feature matrix with shape {X.shape} and target vector with shape {y.shape}.\")\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = train_evaluate_models_with_grid_search(X, y)\n",
    "    \n",
    "    # Save results\n",
    "    save_results_to_csv(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
